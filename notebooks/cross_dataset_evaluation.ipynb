{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== STEP 1. SETUP AND IMPORTS ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-15T07:59:35.737177Z",
     "iopub.status.busy": "2025-10-15T07:59:35.737002Z",
     "iopub.status.idle": "2025-10-15T07:59:36.670081Z",
     "shell.execute_reply": "2025-10-15T07:59:36.669275Z",
     "shell.execute_reply.started": "2025-10-15T07:59:35.737161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Always pull the latest repo to ensure consistency\n",
    "!rm -rf Deep-Learning-Based-Signature-Forgery-Detection-for-Personal-Identity-Authentication-Update\n",
    "!git clone https://github.com/trongjhuongwr/Deep-Learning-Based-Signature-Forgery-Detection-for-Personal-Identity-Authentication-Update.git\n",
    "%cd Deep-Learning-Based-Signature-Forgery-Detection-for-Personal-Identity-Authentication-Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:59:36.672231Z",
     "iopub.status.busy": "2025-10-15T07:59:36.671870Z",
     "iopub.status.idle": "2025-10-15T07:59:45.874029Z",
     "shell.execute_reply": "2025-10-15T07:59:45.873113Z",
     "shell.execute_reply.started": "2025-10-15T07:59:36.672165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "\n",
    "from dataloader.meta_dataloader import SignatureEpisodeDataset\n",
    "from models.feature_extractor import ResNetFeatureExtractor\n",
    "from models.meta_learner import MetricGenerator\n",
    "from utils.model_evaluation import evaluate_meta_model, plot_roc_curve, plot_confusion_matrix\n",
    "from utils.helpers import MemoryTracker\n",
    "\n",
    "print(\"Setup and Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== STEP 2. CONFIGURATION ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:59:45.875406Z",
     "iopub.status.busy": "2025-10-15T07:59:45.874965Z",
     "iopub.status.idle": "2025-10-15T08:00:34.863870Z",
     "shell.execute_reply": "2025-10-15T08:00:34.863133Z",
     "shell.execute_reply.started": "2025-10-15T07:59:45.875379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "K_SHOT = 10\n",
    "N_QUERY_GENUINE = 15\n",
    "N_QUERY_FORGERY = 15\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "BEST_CEDAR_MODEL_DIR = '/kaggle/input/best-cedar-model-weights/best_model_fold_2'  # adjust as needed\n",
    "BHSIG_RAW_BASE_DIR = '/kaggle/input/cedarbhsig-260/'  # adjust if using a different dataset\n",
    "SPLIT_OUTPUT_DIR = '/kaggle/working/'\n",
    "BHSIG_SPLIT_FILE = os.path.join(SPLIT_OUTPUT_DIR, 'bhsig_restructured_split.json')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "NUM_WORKERS = 2 if 'kaggle' in os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '') else 0\n",
    "\n",
    "print(f\"Best CEDAR model path: {BEST_CEDAR_MODEL_DIR}\")\n",
    "print(f\"BHSig raw path: {BHSIG_RAW_BASE_DIR}\")\n",
    "print(f\"Split JSON will be saved to: {BHSIG_SPLIT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== STEP 3. PREPARE BHSIG-260 SPLIT FILE ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“¦ Restructuring BHSig-260 dataset...\")\n",
    "script_path = 'scripts/restructure_bhsig.py'\n",
    "\n",
    "# Run the restructuring script\n",
    "command = (\n",
    "    f\"python {script_path} \"\n",
    "    f\"--base_dir {BHSIG_RAW_BASE_DIR} \"\n",
    "    f\"--output_dir {SPLIT_OUTPUT_DIR} \"\n",
    "    f\"--seed {SEED} \"\n",
    "    f\"--num_bengali 50 \"\n",
    "    f\"--num_hindi 30\"\n",
    ")\n",
    "\n",
    "print(f\"Running command: {command}\")\n",
    "!{command}\n",
    "\n",
    "# Check if the file was created\n",
    "if not os.path.exists(BHSIG_SPLIT_FILE):\n",
    "    raise FileNotFoundError(f\"âŒ Split file not found at {BHSIG_SPLIT_FILE}. Please check script output.\")\n",
    "else:\n",
    "    print(\"BHSig-260 split file generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== STEP 4. LOAD THE BEST MODEL TRAINED ON CEDAR ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Loading best model from {BEST_CEDAR_MODEL_DIR} ---\")\n",
    "\n",
    "feature_extractor = ResNetFeatureExtractor(backbone_name='resnet34', output_dim=512)\n",
    "metric_generator = MetricGenerator(embedding_dim=512)\n",
    "\n",
    "fe_path = os.path.join(BEST_CEDAR_MODEL_DIR, 'best_feature_extractor.pth')\n",
    "mg_path = os.path.join(BEST_CEDAR_MODEL_DIR, 'best_metric_generator.pth')\n",
    "\n",
    "if not os.path.exists(fe_path):\n",
    "    raise FileNotFoundError(f\"Feature extractor weights not found: {fe_path}\")\n",
    "if not os.path.exists(mg_path):\n",
    "    raise FileNotFoundError(f\"Metric generator weights not found: {mg_path}\")\n",
    "\n",
    "feature_extractor.load_state_dict(torch.load(fe_path, map_location=DEVICE))\n",
    "metric_generator.load_state_dict(torch.load(mg_path, map_location=DEVICE))\n",
    "feature_extractor.to(DEVICE)\n",
    "metric_generator.to(DEVICE)\n",
    "\n",
    "print(\"Successfully loaded CEDAR-trained model weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== STEP 5. CREATE BHSIG-260 EVALUATION DATASET ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Creating evaluation dataset for BHSig-260 ---\")\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((220, 150)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "try:\n",
    "    bhsig_test_dataset = SignatureEpisodeDataset(\n",
    "        split_file_path=BHSIG_SPLIT_FILE,\n",
    "        base_data_dir=None,\n",
    "        split_name='meta-test',\n",
    "        k_shot=K_SHOT,\n",
    "        n_query_genuine=N_QUERY_GENUINE,\n",
    "        n_query_forgery=N_QUERY_FORGERY,\n",
    "        augment=False,\n",
    "        use_full_path=True\n",
    "    )\n",
    "    print(f\"Created BHSig-260 evaluation dataset with {len(bhsig_test_dataset)} episodes.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== STEP 6. PERFORM CROSS-DATASET EVALUATION ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Starting Cross-Dataset Evaluation on BHSig-260 ({len(bhsig_test_dataset)} users) ---\")\n",
    "\n",
    "eval_start_time = time.time()\n",
    "\n",
    "if DEVICE.type == 'cuda':\n",
    "    eval_memory_tracker = MemoryTracker(DEVICE)\n",
    "    eval_initial_gpu_mem = eval_memory_tracker.get_used_memory_mb()\n",
    "else:\n",
    "    eval_memory_tracker = None\n",
    "    eval_initial_gpu_mem = 0\n",
    "\n",
    "results_dict, true_labels, predictions, distances = evaluate_meta_model(\n",
    "    feature_extractor,\n",
    "    metric_generator,\n",
    "    bhsig_test_dataset,\n",
    "    DEVICE\n",
    ")\n",
    "\n",
    "eval_duration = time.time() - eval_start_time\n",
    "print(f\"Evaluation finished in {eval_duration:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== STEP 7. REPORT AND VISUALIZE RESULTS ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n--- FINAL CROSS-DATASET RESULTS (CEDAR â†’ BHSig-260) ---\")\n",
    "\n",
    "if results_dict:\n",
    "    for metric, value in results_dict.items():\n",
    "        if metric == 'accuracy':\n",
    "            print(f\"  - {metric.capitalize()}: {value*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"  - {metric.capitalize()}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"Evaluation produced no valid results.\")\n",
    "\n",
    "# Visualizations\n",
    "if len(true_labels) > 0 and len(predictions) > 0 and len(distances) > 0:\n",
    "    print(\"\\nGenerating ROC and Confusion Matrix plots...\")\n",
    "    plot_roc_curve(true_labels, distances, title='ROC Curve - Cross-Dataset (CEDAR â†’ BHSig-260)')\n",
    "    plot_confusion_matrix(true_labels, predictions, title='Confusion Matrix - Cross-Dataset (CEDAR â†’ BHSig-260)')\n",
    "else:\n",
    "    print(\"\\nSkipping visualizations â€” missing evaluation data.\")\n",
    "\n",
    "# Memory usage report\n",
    "if eval_memory_tracker:\n",
    "    eval_final_gpu_mem = eval_memory_tracker.get_used_memory_mb()\n",
    "    print(f\"\\nInitial GPU Mem: {eval_initial_gpu_mem:.2f} MB\")\n",
    "    print(f\"Final GPU Mem: {eval_final_gpu_mem:.2f} MB\")\n",
    "    print(f\"â‰ˆ Used During Eval: {eval_final_gpu_mem - eval_initial_gpu_mem:.2f} MB\")\n",
    "    del eval_memory_tracker\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7043317,
     "sourceId": 11267662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8496202,
     "sourceId": 13389726,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8496752,
     "sourceId": 13390523,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
