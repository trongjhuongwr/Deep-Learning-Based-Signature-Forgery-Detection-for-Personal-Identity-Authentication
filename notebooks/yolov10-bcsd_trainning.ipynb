{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7395829a-e20d-4bf5-b9e8-da3fc2c28b10",
    "_uuid": "16ce85ce-1a84-4c06-9915-12db4aa1e3a9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "**Expected Dataset Structure After Uploading to Kaggle:**\n",
    "```\n",
    "../input/your-dataset-name/\n",
    "‚îú‚îÄ‚îÄ TrainSet/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ X/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_000.jpeg\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ y/  (NOTE: in the code, 'y' is lowercase)\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ y_000.jpeg\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ TestSet/  (Will be merged into training if the corresponding y folder exists)\n",
    "    ‚îú‚îÄ‚îÄ X/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ X_abc.jpeg\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îî‚îÄ‚îÄ y/  (Required if you want to merge TestSet into training)\n",
    "        ‚îú‚îÄ‚îÄ y_abc.jpeg\n",
    "        ‚îî‚îÄ‚îÄ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f12b297c-0534-474d-bb09-ebb8a4da6caa",
    "_uuid": "107efdbe-0d0e-4d10-93e3-d822b356e249",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Environment Setup and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e56d2d4-dbec-4622-b28a-c14414fc5961",
    "_uuid": "bbd0abca-ac43-44d8-8752-256fbe0da005",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T14:43:53.539Z",
     "iopub.status.busy": "2025-05-25T14:43:53.538788Z",
     "iopub.status.idle": "2025-05-25T14:45:13.498124Z",
     "shell.execute_reply": "2025-05-25T14:45:13.497454Z",
     "shell.execute_reply.started": "2025-05-25T14:43:53.538983Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required libraries silently\n",
    "!pip install -q ultralytics opencv-python-headless tqdm\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Image as IPImageDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8ecaf01-693c-4d3e-a017-e72f315d3593",
    "_uuid": "d9262998-1d4d-48fc-b71a-c1fd4a7dbf4a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T14:45:13.499939Z",
     "iopub.status.busy": "2025-05-25T14:45:13.499621Z",
     "iopub.status.idle": "2025-05-25T14:45:13.522602Z",
     "shell.execute_reply": "2025-05-25T14:45:13.521879Z",
     "shell.execute_reply.started": "2025-05-25T14:45:13.499917Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- PATH CONFIGURATION ---\n",
    "KAGGLE_DATASET_NAME = 'bank-checks-signatures-segmentation-dataset'\n",
    "\n",
    "BASE_KAGGLE_INPUT_PATH = f'../input/{KAGGLE_DATASET_NAME}/'\n",
    "\n",
    "ORIGINAL_TRAIN_IMAGES_DIR = os.path.join(BASE_KAGGLE_INPUT_PATH, 'TrainSet/X')\n",
    "ORIGINAL_TRAIN_MASKS_DIR = os.path.join(BASE_KAGGLE_INPUT_PATH, 'TrainSet/y')\n",
    "\n",
    "# Paths for TestSet (images and masks if any for merging into training)\n",
    "ORIGINAL_TEST_IMAGES_DIR = os.path.join(BASE_KAGGLE_INPUT_PATH, 'TestSet/X')\n",
    "ORIGINAL_TEST_MASKS_DIR = os.path.join(BASE_KAGGLE_INPUT_PATH, 'TestSet/y')  # Required if merging TestSet into training\n",
    "\n",
    "# Working directory on Kaggle\n",
    "BASE_WORKING_DIR = '/kaggle/working/'\n",
    "YOLO_DATASET_DIR = os.path.join(BASE_WORKING_DIR, 'signature_yolo_dataset')\n",
    "\n",
    "IMAGES_DIR = os.path.join(YOLO_DATASET_DIR, 'images')\n",
    "LABELS_DIR = os.path.join(YOLO_DATASET_DIR, 'labels')\n",
    "TRAIN_IMAGES_PATH = os.path.join(IMAGES_DIR, 'train')\n",
    "VAL_IMAGES_PATH = os.path.join(IMAGES_DIR, 'val')\n",
    "TRAIN_LABELS_PATH = os.path.join(LABELS_DIR, 'train')\n",
    "VAL_LABELS_PATH = os.path.join(LABELS_DIR, 'val')\n",
    "\n",
    "# Folder to save prediction visualization results (previously named test_set_predictions)\n",
    "# Since TestSet is merged into train, this folder is used to view predictions on any sample images (could be train/val images)\n",
    "VISUALIZATION_PREDICTIONS_SAVE_DIR = os.path.join(BASE_WORKING_DIR, 'predictions_visualization')\n",
    "\n",
    "# Create these directories if they don't exist\n",
    "for path_dir in [YOLO_DATASET_DIR, IMAGES_DIR, LABELS_DIR,\n",
    "                 TRAIN_IMAGES_PATH, VAL_IMAGES_PATH,\n",
    "                 TRAIN_LABELS_PATH, VAL_LABELS_PATH,\n",
    "                 VISUALIZATION_PREDICTIONS_SAVE_DIR]:\n",
    "    os.makedirs(path_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Original TrainSet images path: {ORIGINAL_TRAIN_IMAGES_DIR}\")\n",
    "print(f\"Original TrainSet masks path: {ORIGINAL_TRAIN_MASKS_DIR}\")\n",
    "print(f\"DEBUG: os.path.exists(ORIGINAL_TRAIN_IMAGES_DIR) = {os.path.exists(ORIGINAL_TRAIN_IMAGES_DIR)}\")\n",
    "print(f\"DEBUG: os.path.exists(ORIGINAL_TRAIN_MASKS_DIR) = {os.path.exists(ORIGINAL_TRAIN_MASKS_DIR)}\")\n",
    "\n",
    "print(f\"Original TestSet images path (for merging/visualizing): {ORIGINAL_TEST_IMAGES_DIR}\")\n",
    "print(f\"DEBUG: os.path.exists(ORIGINAL_TEST_IMAGES_DIR) = {os.path.exists(ORIGINAL_TEST_IMAGES_DIR)}\")\n",
    "print(f\"Original TestSet masks path (for merging): {ORIGINAL_TEST_MASKS_DIR}\")\n",
    "print(f\"DEBUG: os.path.exists(ORIGINAL_TEST_MASKS_DIR) = {os.path.exists(ORIGINAL_TEST_MASKS_DIR)}\")\n",
    "\n",
    "print(f\"YOLO dataset folder will be created at: {YOLO_DATASET_DIR}\")\n",
    "print(f\"Prediction visualization save folder: {VISUALIZATION_PREDICTIONS_SAVE_DIR}\")\n",
    "\n",
    "# Check existence of basic TrainSet directories\n",
    "if not os.path.isdir(ORIGINAL_TRAIN_IMAGES_DIR):\n",
    "    assert False, f\"ERROR: TrainSet/X directory not found at: {ORIGINAL_TRAIN_IMAGES_DIR}\"\n",
    "if not os.path.isdir(ORIGINAL_TRAIN_MASKS_DIR):\n",
    "    assert False, f\"ERROR: TrainSet/y directory not found at: {ORIGINAL_TRAIN_MASKS_DIR}\"\n",
    "\n",
    "# Configure merging TestSet into training\n",
    "MERGE_TEST_SET_INTO_TRAINING = True  # Set to True to merge TestSet, False to use only TrainSet\n",
    "\n",
    "if MERGE_TEST_SET_INTO_TRAINING:\n",
    "    print(\"\\n===> MODE: Merging TestSet (images and masks) into training data <===\")\n",
    "    if not os.path.isdir(ORIGINAL_TEST_IMAGES_DIR):\n",
    "        print(f\"WARNING: TestSet/X directory not found at: {ORIGINAL_TEST_IMAGES_DIR}. Cannot merge TestSet.\")\n",
    "        MERGE_TEST_SET_INTO_TRAINING = False\n",
    "    if not os.path.isdir(ORIGINAL_TEST_MASKS_DIR):\n",
    "        print(f\"WARNING: TestSet/y directory not found at: {ORIGINAL_TEST_MASKS_DIR} (required for masks). Cannot merge TestSet.\")\n",
    "        MERGE_TEST_SET_INTO_TRAINING = False\n",
    "    if MERGE_TEST_SET_INTO_TRAINING:\n",
    "        print(\"TestSet information is valid for merging.\")\n",
    "else:\n",
    "    print(\"\\n===> MODE: Using only TrainSet for training <===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e589ba74-5802-4a42-a516-86fd43e55060",
    "_uuid": "5624d353-4e09-485b-b31c-473eaa33397a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Data Preprocessing \n",
    "(Merge TrainSet and TestSet if configured)  \n",
    "#  \n",
    "This section will:  \n",
    "1. Collect image-mask pairs from `TrainSet` and (if requested) from `TestSet`.  \n",
    "2. Use OpenCV to find bounding boxes from the masks.  \n",
    "3. Convert bounding box coordinates into YOLO format.  \n",
    "4. Copy images and create `.txt` label files in the working directory.  \n",
    "5. Split the merged dataset into training (`train`) and validation (`val`) sets.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "39f0a506-ba7c-4da6-b268-a666359b1c40",
    "_uuid": "d5e6cf6f-73a4-4204-996b-d52b8e9047d2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T14:45:13.523532Z",
     "iopub.status.busy": "2025-05-25T14:45:13.523319Z",
     "iopub.status.idle": "2025-05-25T14:45:23.268034Z",
     "shell.execute_reply": "2025-05-25T14:45:23.267453Z",
     "shell.execute_reply.started": "2025-05-25T14:45:13.523515Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_mask_to_yolo_format(mask_image_path, original_image_width, original_image_height):\n",
    "    \"\"\"\n",
    "    Convert a mask image file into a list of bounding boxes in YOLO format.\n",
    "    \"\"\"\n",
    "    mask = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if mask is None:\n",
    "        print(f\"Warning (convert_mask): Could not read mask file: {mask_image_path}\")\n",
    "        return []\n",
    "\n",
    "    threshold_value = 127  # Threshold for binarizing the mask\n",
    "    _, thresh = cv2.threshold(mask, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    yolo_annotations = []\n",
    "    for contour_idx, contour in enumerate(contours):\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area < 50:  # Filter out very small objects (possibly noise) - adjust if needed\n",
    "            continue\n",
    "        x_min, y_min, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        if w == 0 or h == 0:  # Skip invalid bounding boxes\n",
    "            continue\n",
    "\n",
    "        # Convert to YOLO format: (class_id x_center y_center width height) - normalized\n",
    "        x_center = (x_min + w / 2) / original_image_width\n",
    "        y_center = (y_min + h / 2) / original_image_height\n",
    "        norm_width = w / original_image_width\n",
    "        norm_height = h / original_image_height\n",
    "        \n",
    "        # Assume only one class 'signature' with class_id = 0\n",
    "        yolo_annotations.append(f\"0 {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\")\n",
    "    return yolo_annotations\n",
    "\n",
    "def prepare_data_for_yolo(image_mask_pairs_list):\n",
    "    all_prepared_image_paths = []\n",
    "    all_prepared_label_paths = []\n",
    "    # Create temporary directories to store processed images and labels before splitting into train/val\n",
    "    temp_image_dir = os.path.join(YOLO_DATASET_DIR, \"_temp_images_combined\") \n",
    "    temp_label_dir = os.path.join(YOLO_DATASET_DIR, \"_temp_labels_combined\")\n",
    "    os.makedirs(temp_image_dir, exist_ok=True)\n",
    "    os.makedirs(temp_label_dir, exist_ok=True)\n",
    "\n",
    "    for img_path, mask_path in tqdm(image_mask_pairs_list, desc=\"Preparing YOLO data\"):\n",
    "        img_filename_no_ext = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        \n",
    "        try:  # Prefer reading with PIL to get dimensions\n",
    "            with Image.open(img_path) as pil_img:\n",
    "                img_width, img_height = pil_img.size\n",
    "        except Exception as e:\n",
    "            print(f\"Warning (prepare_data): Error reading dimensions of {img_path} with PIL: {e}. Trying with OpenCV.\")\n",
    "            cv_img = cv2.imread(img_path)\n",
    "            if cv_img is not None:\n",
    "                img_height, img_width = cv_img.shape[:2]  # (height, width, channels)\n",
    "            else:\n",
    "                print(f\"ERROR (prepare_data): Could not read image {img_path} with either PIL or OpenCV. Skipping.\")\n",
    "                continue\n",
    "        \n",
    "        if img_width == 0 or img_height == 0:\n",
    "            print(f\"ERROR (prepare_data): Invalid image dimensions (0) for {img_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        yolo_annotations = convert_mask_to_yolo_format(mask_path, img_width, img_height)\n",
    "        \n",
    "        if yolo_annotations:\n",
    "            # Copy original image to temporary directory\n",
    "            dest_image_path = os.path.join(temp_image_dir, os.path.basename(img_path))\n",
    "            shutil.copy2(img_path, dest_image_path) \n",
    "            \n",
    "            # Create YOLO label file\n",
    "            label_file_path = os.path.join(temp_label_dir, f\"{img_filename_no_ext}.txt\")\n",
    "            with open(label_file_path, 'w') as f:\n",
    "                for line in yolo_annotations:\n",
    "                    f.write(line + \"\\n\")\n",
    "            \n",
    "            all_prepared_image_paths.append(dest_image_path)\n",
    "            all_prepared_label_paths.append(label_file_path)\n",
    "            \n",
    "    print(f\"Processed {len(all_prepared_image_paths)} images with valid annotations.\")\n",
    "    return all_prepared_image_paths, all_prepared_label_paths\n",
    "\n",
    "# --- Collect image and mask pairs ---\n",
    "image_mask_pairs = []  # Will contain (image_path, mask_path) pairs\n",
    "\n",
    "def collect_pairs_from_set(set_name, images_dir, masks_dir, existing_pairs_list):\n",
    "    \"\"\"Helper function to collect (image, mask) pairs from a source directory.\"\"\"\n",
    "    print(f\"\\nCollecting data from {set_name}...\")\n",
    "    if not os.path.isdir(images_dir):\n",
    "        print(f\"WARNING: Image directory '{images_dir}' for {set_name} does not exist. Skipping.\")\n",
    "        return\n",
    "    if not os.path.isdir(masks_dir):\n",
    "        print(f\"WARNING: Mask directory '{masks_dir}' for {set_name} does not exist. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Get all .jpeg or .jpg images, case-insensitive for X/x\n",
    "    # Pattern '[Xx]_*.[jp][pn]g' will match X_*.jpg, X_*.jpeg, x_*.jpg, x_*.jpeg\n",
    "    image_paths_in_set = glob.glob(os.path.join(images_dir, '[Xx]_*.[jJ][pP][eE][gG]')) + \\\n",
    "                         glob.glob(os.path.join(images_dir, '[Xx]_*.[jJ][pP][gG]'))\n",
    "\n",
    "    if not image_paths_in_set:\n",
    "        print(f\"No image files found matching pattern '[Xx]_*.(jpeg|jpg)' in {images_dir} for {set_name}.\")\n",
    "        return\n",
    "\n",
    "    collected_count = 0\n",
    "    for img_path in tqdm(image_paths_in_set, desc=f\"Processing {set_name}\"):\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        \n",
    "        # Create corresponding mask filename: y_....ext from X_....ext or x_....ext\n",
    "        # Keep the original image file extension for the mask (e.g., .jpeg or .jpg)\n",
    "        img_name_part = img_filename[2:]  # Remove 'X_' or 'x_'\n",
    "        mask_filename = \"y_\" + img_name_part\n",
    "        \n",
    "        mask_path = os.path.join(masks_dir, mask_filename)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            existing_pairs_list.append((img_path, mask_path))\n",
    "            collected_count += 1\n",
    "    print(f\"Found {collected_count} valid image-mask pairs in {set_name}.\")\n",
    "\n",
    "# Collect from TrainSet (always performed)\n",
    "collect_pairs_from_set(\"TrainSet\", ORIGINAL_TRAIN_IMAGES_DIR, ORIGINAL_TRAIN_MASKS_DIR, image_mask_pairs)\n",
    "\n",
    "# Collect from TestSet (to merge into training) if MERGE_TEST_SET_INTO_TRAINING is True\n",
    "if MERGE_TEST_SET_INTO_TRAINING:\n",
    "    collect_pairs_from_set(\"TestSet (merged into train)\", ORIGINAL_TEST_IMAGES_DIR, ORIGINAL_TEST_MASKS_DIR, image_mask_pairs)\n",
    "\n",
    "print(f\"\\nTotal of {len(image_mask_pairs)} image-mask pairs (before deduplication).\")\n",
    "\n",
    "# Remove duplicate pairs based on image paths (important if TestSet may overlap with TrainSet)\n",
    "if image_mask_pairs:\n",
    "    unique_image_paths_seen = set()\n",
    "    unique_pairs = []\n",
    "    for img_p, msk_p in image_mask_pairs:\n",
    "        if img_p not in unique_image_paths_seen:\n",
    "            unique_image_paths_seen.add(img_p)\n",
    "            unique_pairs.append((img_p, msk_p))\n",
    "    \n",
    "    if len(unique_pairs) < len(image_mask_pairs):\n",
    "        print(f\"Removed {len(image_mask_pairs) - len(unique_pairs)} duplicate pairs. Unique pairs remaining: {len(unique_pairs)}\")\n",
    "    image_mask_pairs = unique_pairs\n",
    "    print(f\"Number of unique image-mask pairs to process: {len(image_mask_pairs)}\")\n",
    "\n",
    "# Process data (prepare_data_for_yolo will work with merged and filtered image_mask_pairs)\n",
    "all_images, all_labels = [], []\n",
    "if image_mask_pairs:\n",
    "    all_images, all_labels = prepare_data_for_yolo(image_mask_pairs)\n",
    "else:\n",
    "    print(\"ERROR: No image-mask pairs collected. Stopping processing.\")\n",
    "    assert False, \"No input data available.\"\n",
    "\n",
    "# --- Split train/val from combined data ---\n",
    "val_images_list = []  # Initialized for use in data.yaml and later checks\n",
    "train_images_list = []\n",
    "\n",
    "if all_images and all_labels:\n",
    "    assert len(all_images) == len(all_labels), \"ERROR: Number of images and labels do not match after preparation!\"\n",
    "    \n",
    "    # Sort to ensure consistency (although train_test_split shuffles)\n",
    "    all_images.sort()\n",
    "    all_labels.sort()\n",
    "    \n",
    "    val_size_percentage = 0.2  # Validation set percentage (20%)\n",
    "    \n",
    "    # Adjust val_size for small datasets\n",
    "    if len(all_images) < 10: \n",
    "        val_size_percentage = 0.1 if len(all_images) > 5 else 0  # No val if <= 5 images total\n",
    "        if val_size_percentage == 0:\n",
    "            print(\"Warning: Too few images (<=5), no separate validation set will be created.\")\n",
    "        \n",
    "    if val_size_percentage > 0 and len(all_images) * val_size_percentage >= 1:  # Ensure at least 1 val sample\n",
    "        train_images_list, val_images_list, train_labels_list, val_labels_list = train_test_split(\n",
    "            all_images, all_labels, test_size=val_size_percentage, random_state=42, shuffle=True\n",
    "        )\n",
    "    else:  # Not enough data for validation or val_size_percentage = 0\n",
    "        train_images_list = all_images\n",
    "        train_labels_list = all_labels\n",
    "        val_images_list, val_labels_list = [], []  # No validation set\n",
    "        print(\"All data used for training set. No separate validation set.\")\n",
    "\n",
    "    def copy_files_to_final_yolo_dir(source_file_list, destination_folder):\n",
    "        os.makedirs(destination_folder, exist_ok=True)\n",
    "        copied_count = 0\n",
    "        for f_path in tqdm(source_file_list, desc=f\"Copying to {os.path.basename(destination_folder)}\"):\n",
    "            try:\n",
    "                shutil.copy(f_path, os.path.join(destination_folder, os.path.basename(f_path)))\n",
    "                copied_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {f_path} to {destination_folder}: {e}\")\n",
    "        print(f\"Copied {copied_count} files to {destination_folder}.\")\n",
    "\n",
    "    copy_files_to_final_yolo_dir(train_images_list, TRAIN_IMAGES_PATH)\n",
    "    copy_files_to_final_yolo_dir(train_labels_list, TRAIN_LABELS_PATH)\n",
    "    \n",
    "    if val_images_list:  # Only copy if validation set exists\n",
    "        copy_files_to_final_yolo_dir(val_images_list, VAL_IMAGES_PATH)\n",
    "        copy_files_to_final_yolo_dir(val_labels_list, VAL_LABELS_PATH)\n",
    "        \n",
    "    print(\"\\nCompleted preparation and splitting of train/val data for YOLO.\")\n",
    "    print(f\"Number of training images: {len(os.listdir(TRAIN_IMAGES_PATH))}, Number of training labels: {len(os.listdir(TRAIN_LABELS_PATH))}\")\n",
    "    if val_images_list:\n",
    "        print(f\"Number of validation images: {len(os.listdir(VAL_IMAGES_PATH))}, Number of validation labels: {len(os.listdir(VAL_LABELS_PATH))}\")\n",
    "    else:\n",
    "        print(\"No validation set created.\")\n",
    "        \n",
    "    # Clean up temporary directories after copying\n",
    "    temp_combined_images = os.path.join(YOLO_DATASET_DIR, \"_temp_images_combined\")\n",
    "    temp_combined_labels = os.path.join(YOLO_DATASET_DIR, \"_temp_labels_combined\")\n",
    "    if os.path.exists(temp_combined_images):\n",
    "        shutil.rmtree(temp_combined_images)\n",
    "        print(f\"Deleted temporary directory: {temp_combined_images}\")\n",
    "    if os.path.exists(temp_combined_labels):\n",
    "        shutil.rmtree(temp_combined_labels)\n",
    "        print(f\"Deleted temporary directory: {temp_combined_labels}\")\n",
    "else:\n",
    "    print(\"ERROR: No data prepared (all_images empty). Cannot continue.\")\n",
    "    assert False, \"No data available for training.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69b94480-155e-4a81-808f-fde4cc6cb614",
    "_uuid": "258da716-d7df-4422-888f-5f745abe032c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Create file `data.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "654c4cdb-b82e-49d1-9f7b-ae8550dce35b",
    "_uuid": "920438fe-1dac-4b5d-9abf-f9d4367cd82e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T14:45:23.269019Z",
     "iopub.status.busy": "2025-05-25T14:45:23.268791Z",
     "iopub.status.idle": "2025-05-25T14:45:23.275207Z",
     "shell.execute_reply": "2025-05-25T14:45:23.274545Z",
     "shell.execute_reply.started": "2025-05-25T14:45:23.269001Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Determine the val path for the yaml file\n",
    "# If val_images_list is empty (no validation set), point val path to train path\n",
    "# or better, point to VAL_IMAGES_PATH (which will be an empty directory)\n",
    "# YOLO typically handles an empty val directory by skipping that validation epoch.\n",
    "val_path_for_yaml_file = VAL_IMAGES_PATH\n",
    "if not val_images_list:\n",
    "    print(\"Warning: No validation set. In data.yaml, 'val' will point to an empty validation directory or the train directory.\")\n",
    "    # Some YOLO versions may require a valid val path even if empty.\n",
    "    # Alternatively, if you want YOLO to validate on the train set when no separate val set exists:\n",
    "    # val_path_for_yaml_file = TRAIN_IMAGES_PATH \n",
    "    # However, pointing to VAL_IMAGES_PATH (even if empty) is usually the clearer approach.\n",
    "    if not os.path.exists(VAL_IMAGES_PATH) or not os.listdir(VAL_IMAGES_PATH):\n",
    "         print(f\"VAL_IMAGES_PATH directory ('{VAL_IMAGES_PATH}') is empty or does not exist. The model will not perform validation epochs.\")\n",
    "\n",
    "\n",
    "yaml_content_str = f\"\"\"\n",
    "train: {TRAIN_IMAGES_PATH}\n",
    "val: {val_path_for_yaml_file}\n",
    "\n",
    "# number of classes\n",
    "nc: 1\n",
    "\n",
    "# class names\n",
    "names: ['signature']\n",
    "\"\"\"\n",
    "yaml_file_path_str = os.path.join(BASE_WORKING_DIR, 'signature_data.yaml')\n",
    "with open(yaml_file_path_str, 'w') as f:\n",
    "    f.write(yaml_content_str)\n",
    "print(f\"\\nThe signature_data.yaml file has been created at: {yaml_file_path_str}\")\n",
    "print(\"Content:\")\n",
    "print(yaml_content_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cc4d57b1-5079-400e-9685-f07f65709051",
    "_uuid": "ef1a4a84-b67f-41ef-a01a-334e86e7e8f8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Train YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "55290a68-4ee7-47e3-bc31-8535e6475b5f",
    "_uuid": "fbbf3ded-365f-4c06-8e2e-2378979bb5ab",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T15:38:58.606713Z",
     "iopub.status.busy": "2025-05-25T15:38:58.606453Z",
     "iopub.status.idle": "2025-05-25T15:43:06.943302Z",
     "shell.execute_reply": "2025-05-25T15:43:06.94254Z",
     "shell.execute_reply.started": "2025-05-25T15:38:58.606698Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Training Configuration ---\n",
    "# You can try other YOLOv10 models: yolov10n, yolov10s, yolov10m, yolov10b, yolov10l, yolov10x\n",
    "CHOSEN_MODEL_NAME = 'yolov10s.pt'  # Smaller model, faster training, suitable for experimentation\n",
    "NUM_EPOCHS = 100                   # Number of epochs (increase, e.g., 50, 100, 200 for better results on large datasets)\n",
    "TRAIN_BATCH_SIZE = 16             # Batch size (adjust based on GPU memory, e.g., 8, 16, 32)\n",
    "INPUT_IMG_SIZE = 640              # Input image size for the model (typically 640 for YOLO)\n",
    "\n",
    "# Project and run names for organizing results\n",
    "PROJECT_SAVE_NAME = 'Signature_YOLOv10_Detection_Kaggle'\n",
    "RUN_SAVE_NAME = f'{os.path.splitext(CHOSEN_MODEL_NAME)[0]}_epochs{NUM_EPOCHS}_batch{TRAIN_BATCH_SIZE}'\n",
    "\n",
    "# Check the training directory before starting\n",
    "if not os.path.exists(TRAIN_IMAGES_PATH) or not os.listdir(TRAIN_IMAGES_PATH):\n",
    "    print(f\"ERROR: No images found in training directory: {TRAIN_IMAGES_PATH} or directory does not exist.\")\n",
    "    print(\"Please check the data preparation steps.\")\n",
    "    assert False, \"Training directory is empty or does not exist.\"\n",
    "\n",
    "print(f\"\\nStarting model training: {CHOSEN_MODEL_NAME}\")\n",
    "print(f\"Total Epochs: {NUM_EPOCHS}, Batch Size: {TRAIN_BATCH_SIZE}, Image Size: {INPUT_IMG_SIZE}\")\n",
    "print(f\"Project will be saved at: {PROJECT_SAVE_NAME}, Run Name: {RUN_SAVE_NAME}\")\n",
    "\n",
    "# Initialize YOLO model (will automatically download pre-trained weights if not present)\n",
    "yolo_model = YOLO(CHOSEN_MODEL_NAME) \n",
    "training_results = None  # Initialize variable to store training results\n",
    "\n",
    "try:\n",
    "    training_results = yolo_model.train(\n",
    "        data=yaml_file_path_str,    # Path to data.yaml file\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch=TRAIN_BATCH_SIZE,\n",
    "        imgsz=INPUT_IMG_SIZE,\n",
    "        project=os.path.join(BASE_WORKING_DIR, PROJECT_SAVE_NAME),  # Root directory for saving project\n",
    "        name=RUN_SAVE_NAME,         # Subdirectory name for this run\n",
    "        patience=20,                # Early stopping if no improvement after 20 epochs (for val loss)\n",
    "        exist_ok=True,              # Allow overwriting previous run with same name (useful for debugging)\n",
    "        # workers=2,                # Number of data loading threads, adjust if needed\n",
    "        # device=0,                 # Specify GPU (0) or CPU ('cpu')\n",
    "        # val=True,                 # Default is True, will run validation after each epoch if val set exists\n",
    "    )\n",
    "    print(\"\\nüéâ Training completed!\")\n",
    "    if training_results:\n",
    "        print(f\"Results and model saved at: {training_results.save_dir}\")  # training_results.save_dir is the exact path\n",
    "\n",
    "    # Evaluate model on validation set if it exists and training was successful\n",
    "    if val_images_list and training_results:  # Only evaluate if validation set exists and training was successful\n",
    "        print(\"\\nEvaluating model on validation set...\")\n",
    "        # Path to the best model after training\n",
    "        best_trained_model_path = os.path.join(training_results.save_dir, 'weights/best.pt')\n",
    "        if os.path.exists(best_trained_model_path):\n",
    "            final_trained_model = YOLO(best_trained_model_path)\n",
    "            validation_metrics = final_trained_model.val()  # Will use val path from data.yaml by default\n",
    "            print(\"Evaluation on validation set completed.\")\n",
    "            # Print key metrics, for example:\n",
    "            print(f\"  mAP50-95: {validation_metrics.box.map:.4f}\")\n",
    "            print(f\"  mAP50:    {validation_metrics.box.map50:.4f}\")\n",
    "            # print(f\"  mAP75:    {validation_metrics.box.map75:.4f}\")\n",
    "        else:\n",
    "            print(f\"ERROR: Best model not found at '{best_trained_model_path}' for evaluation.\")\n",
    "    elif not val_images_list:\n",
    "        print(\"\\nNote: No separate validation set available for automatic evaluation after training.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"A serious error occurred during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6b3c504-8074-4de5-9f38-9203a122ccd4",
    "_uuid": "57925aa7-f139-49cd-af75-b90ed5797f02",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## View Training Results\n",
    "#\n",
    "-   Training results (including model weights, charts, etc.) will be located in the directory:\n",
    "    `/kaggle/working/PROJECT_SAVE_NAME/RUN_SAVE_NAME/`\n",
    "-   You can download these files from the \"Output\" section of the Kaggle Notebook after committing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a6d00a8c-7ba9-48d7-8d77-d626646c317f",
    "_uuid": "4d17b23e-4f9c-4b83-8a5f-407e8fd7069d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T15:43:06.945725Z",
     "iopub.status.busy": "2025-05-25T15:43:06.945243Z",
     "iopub.status.idle": "2025-05-25T15:43:06.982532Z",
     "shell.execute_reply": "2025-05-25T15:43:06.981872Z",
     "shell.execute_reply.started": "2025-05-25T15:43:06.94569Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display training result charts if training has been run and results are available\n",
    "# Ensure PROJECT_SAVE_NAME and RUN_SAVE_NAME are defined from the training cell\n",
    "if 'PROJECT_SAVE_NAME' in locals() and 'RUN_SAVE_NAME' in locals() and 'BASE_WORKING_DIR' in locals():\n",
    "    # Try to get the path from training_results if available, otherwise construct manually\n",
    "    if 'training_results' in locals() and training_results and hasattr(training_results, 'save_dir'):\n",
    "        run_specific_output_path = training_results.save_dir\n",
    "    else:  # Construct manually if training_results is unavailable (e.g., due to partial failure)\n",
    "        run_specific_output_path = os.path.join(BASE_WORKING_DIR, PROJECT_SAVE_NAME, RUN_SAVE_NAME)\n",
    "\n",
    "    if os.path.exists(run_specific_output_path):\n",
    "        print(f\"Training results directory: {run_specific_output_path}\")\n",
    "        \n",
    "        # Display results.png chart (contains loss curves, mAP curves, etc.)\n",
    "        results_plot_png_path = os.path.join(run_specific_output_path, 'results.png')\n",
    "        if os.path.exists(results_plot_png_path):\n",
    "            print(\"\\nTraining summary chart (results.png):\")\n",
    "            display(IPImageDisplay(filename=results_plot_png_path, width=900))  # Increase width for better visibility\n",
    "        else:\n",
    "            print(\"\\n'results.png' file not found in the results directory.\")\n",
    "\n",
    "        # Display prediction images on a batch from the validation set (if val set exists)\n",
    "        if val_images_list:  # Check if there is a validation set\n",
    "            val_batch_prediction_image_path = os.path.join(run_specific_output_path, 'val_batch0_pred.jpg')\n",
    "            if os.path.exists(val_batch_prediction_image_path):\n",
    "                print(\"\\nPrediction images on a batch from the validation set (val_batch0_pred.jpg):\")\n",
    "                display(IPImageDisplay(filename=val_batch_prediction_image_path, width=900))\n",
    "            # else:\n",
    "                # print(\"\\n'val_batch0_pred.jpg' not found. This may be due to no validation set or an error during saving.\")\n",
    "        \n",
    "        # Or display an image from a training batch if no validation batch image is available\n",
    "        else:  # If there is no val_images_list (i.e., no validation set)\n",
    "            train_batch_image_path = os.path.join(run_specific_output_path, 'train_batch0.jpg')\n",
    "            if os.path.exists(train_batch_image_path):\n",
    "                print(\"\\nImage from a training batch (train_batch0.jpg):\")\n",
    "                display(IPImageDisplay(filename=train_batch_image_path, width=900))\n",
    "            # else:\n",
    "            #     print(\"\\nNo sample batch image from training ('train_batch0.jpg') found.\")\n",
    "        \n",
    "        # Confusion matrix (if available)\n",
    "        confusion_matrix_path = os.path.join(run_specific_output_path, 'confusion_matrix.png')\n",
    "        if os.path.exists(confusion_matrix_path):\n",
    "            print(\"\\nConfusion Matrix on the validation set:\")\n",
    "            display(IPImageDisplay(filename=confusion_matrix_path, width=700))\n",
    "\n",
    "    else:  # If run_specific_output_path does not exist\n",
    "        print(f\"Results directory '{run_specific_output_path}' does not exist.\")\n",
    "        print(\"This may occur if training has not been performed, failed midway, or project/run names were changed.\")\n",
    "else:\n",
    "    print(\"PROJECT_SAVE_NAME or RUN_SAVE_NAME variables are not defined from the training cell. Cannot display results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b24da790-4921-4168-ba6f-1cd8fe069798",
    "_uuid": "69a93ad9-ea0b-4b89-aa51-d5292032af5a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Visualize Predictions on Some Images (Optional) \n",
    "#\n",
    "**IMPORTANT NOTE:**\n",
    "If you have merged `TestSet/X` and `TestSet/y` into the training data (via the flag `MERGE_TEST_SET_INTO_TRAINING = True`),\n",
    "then `ORIGINAL_TEST_IMAGES_DIR` (if used below as the image source) will contain images **ALREADY USED IN TRAINING**.\n",
    "Predicting on these images is not a true \"test\" on unseen data.\n",
    "#\n",
    "This section is primarily for visually inspecting the model's results on specific images (which could be from the train set, validation set, or a completely new set if you provide a different path).\n",
    "The results (images with drawn bounding boxes) will be saved to the configured directory (`VISUALIZATION_PREDICTIONS_SAVE_DIR`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c60122ed-8b88-4a26-83bc-9806286b9c63",
    "_uuid": "b332b7f3-9099-4bf9-a692-a914adfdd62f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T15:43:06.983567Z",
     "iopub.status.busy": "2025-05-25T15:43:06.983308Z",
     "iopub.status.idle": "2025-05-25T15:43:08.017048Z",
     "shell.execute_reply": "2025-05-25T15:43:08.016399Z",
     "shell.execute_reply.started": "2025-05-25T15:43:06.983551Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "run_visualization_prediction = True  # Set to False if you don't want to run this section\n",
    "\n",
    "# --- Determine the best model to use for visualization ---\n",
    "best_model_for_viz_path = \"\"\n",
    "if 'training_results' in locals() and training_results and hasattr(training_results, 'save_dir'):\n",
    "    potential_model_path_viz = os.path.join(training_results.save_dir, 'weights/best.pt')\n",
    "    if os.path.exists(potential_model_path_viz):\n",
    "        best_model_for_viz_path = potential_model_path_viz\n",
    "        print(f\"Using 'best.pt' model from the current training for visualization: {best_model_for_viz_path}\")\n",
    "\n",
    "if not best_model_for_viz_path:  # If not found from training_results (e.g., running notebook only for prediction)\n",
    "    if 'PROJECT_SAVE_NAME' in locals() and 'RUN_SAVE_NAME' in locals() and 'BASE_WORKING_DIR' in locals():\n",
    "        potential_model_path_viz = os.path.join(BASE_WORKING_DIR, PROJECT_SAVE_NAME, RUN_SAVE_NAME, 'weights/best.pt')\n",
    "        if os.path.exists(potential_model_path_viz):\n",
    "            best_model_for_viz_path = potential_model_path_viz\n",
    "            print(f\"Using 'best.pt' model found at saved path for visualization: {best_model_for_viz_path}\")\n",
    "        else:\n",
    "            print(f\"Could not find 'best.pt' model at expected path: '{potential_model_path_viz}'.\")\n",
    "            run_visualization_prediction = False\n",
    "    else:\n",
    "        print(\"Required variables (PROJECT_SAVE_NAME, etc.) to locate saved model are not defined.\")\n",
    "        run_visualization_prediction = False\n",
    "\n",
    "if not best_model_for_viz_path and run_visualization_prediction:  # Double-check\n",
    "    print(\"Could not identify 'best.pt' model for visualization. Skipping.\")\n",
    "    run_visualization_prediction = False\n",
    "\n",
    "# --- Select image source for visualization ---\n",
    "source_images_for_visualization = []\n",
    "if run_visualization_prediction:\n",
    "    # Prioritize selecting a few random images from the validation set if available\n",
    "    if val_images_list:\n",
    "        print(f\"Selecting sample images from validation set ({VAL_IMAGES_PATH}) for visualization.\")\n",
    "        # val_images_list contains paths to images in _temp, need to use VAL_IMAGES_PATH\n",
    "        actual_val_image_files = glob.glob(os.path.join(VAL_IMAGES_PATH, '*.*'))\n",
    "        if actual_val_image_files:\n",
    "             source_images_for_visualization = random.sample(actual_val_image_files, min(len(actual_val_image_files), 5))  # Max 5 images\n",
    "    # If no validation set, try using training set\n",
    "    elif train_images_list:\n",
    "        print(f\"No validation set. Selecting sample images from training set ({TRAIN_IMAGES_PATH}) for visualization.\")\n",
    "        actual_train_image_files = glob.glob(os.path.join(TRAIN_IMAGES_PATH, '*.*'))\n",
    "        if actual_train_image_files:\n",
    "            source_images_for_visualization = random.sample(actual_train_image_files, min(len(actual_train_image_files), 5))\n",
    "    # If you want to use ORIGINAL_TEST_IMAGES_DIR (images merged into train if MERGE_TEST_SET_INTO_TRAINING=True)\n",
    "    # elif os.path.isdir(ORIGINAL_TEST_IMAGES_DIR) and os.listdir(ORIGINAL_TEST_IMAGES_DIR):\n",
    "    #     print(f\"Selecting sample images from ORIGINAL_TEST_IMAGES_DIR ('{ORIGINAL_TEST_IMAGES_DIR}') for visualization.\")\n",
    "    #     all_orig_test_imgs = glob.glob(os.path.join(ORIGINAL_TEST_IMAGES_DIR, '[Xx]_*.*'))\n",
    "    #     if all_orig_test_imgs:\n",
    "    #         source_images_for_visualization = random.sample(all_orig_test_imgs, min(len(all_orig_test_imgs), 5))\n",
    "            \n",
    "    if not source_images_for_visualization:\n",
    "        print(\"No suitable images found for visualization. Skipping this section.\")\n",
    "        run_visualization_prediction = False\n",
    "\n",
    "if run_visualization_prediction:\n",
    "    print(f\"\\nStarting visualization of predictions on {len(source_images_for_visualization)} sample images...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure required configuration variables exist (from training cell)\n",
    "        current_input_img_size_viz = INPUT_IMG_SIZE if 'INPUT_IMG_SIZE' in locals() else 640\n",
    "        current_model_name_viz = os.path.splitext(CHOSEN_MODEL_NAME)[0] if 'CHOSEN_MODEL_NAME' in locals() else \"yolov10_model\"\n",
    "            \n",
    "        visualization_model = YOLO(best_model_for_viz_path)\n",
    "\n",
    "        # Subdirectory name for this visualization run\n",
    "        visualization_run_name = f\"{current_model_name_viz}_viz_predictions\"\n",
    "        \n",
    "        # Perform predictions on the selected image list\n",
    "        # Source can be a list of image paths\n",
    "        visualization_model.predict(\n",
    "            source=source_images_for_visualization, \n",
    "            imgsz=current_input_img_size_viz,\n",
    "            conf=0.35,      # Confidence threshold (can be increased for fewer boxes)\n",
    "            iou=0.5,        # IoU threshold\n",
    "            save=True,      # Save images with drawn bounding boxes\n",
    "            project=VISUALIZATION_PREDICTIONS_SAVE_DIR, \n",
    "            name=visualization_run_name,     \n",
    "            exist_ok=True,\n",
    "            show_labels=True,  # Show class labels\n",
    "            show_conf=True     # Show confidence scores\n",
    "        )\n",
    "        \n",
    "        actual_visualization_output_dir = os.path.join(VISUALIZATION_PREDICTIONS_SAVE_DIR, visualization_run_name)\n",
    "        print(f\"Visualization completed. Results (images with bounding boxes) saved at: {actual_visualization_output_dir}\")\n",
    "\n",
    "        # Display predicted images (located in the subdirectory of actual_visualization_output_dir)\n",
    "        predicted_viz_image_files = []\n",
    "        # Images are typically saved directly in the subdirectory (no further subdirectories when source is a list)\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "            predicted_viz_image_files.extend(glob.glob(os.path.join(actual_visualization_output_dir, ext)))\n",
    "        \n",
    "        # Filter out non-image files if any (e.g., labels.jpg if generated by YOLO)\n",
    "        predicted_viz_image_files = [f for f in predicted_viz_image_files if not os.path.basename(f).startswith('labels')]\n",
    "\n",
    "        if predicted_viz_image_files:\n",
    "            print(f\"\\nDisplaying visualized images ({len(predicted_viz_image_files)} images):\")\n",
    "            for img_file_viz in sorted(predicted_viz_image_files):  # Sort for easier tracking\n",
    "                print(f\"Predicted image: {img_file_viz}\")\n",
    "                display(IPImageDisplay(filename=img_file_viz, width=700)) \n",
    "        else:\n",
    "            print(f\"No result images found in '{actual_visualization_output_dir}' to display.\")\n",
    "            \n",
    "    except NameError as ne_viz:\n",
    "        print(f\"NameError occurred while attempting visualization: {ne_viz}\")\n",
    "        print(\"Ensure required variables (e.g., 'YOLO', 'INPUT_IMG_SIZE', 'CHOSEN_MODEL_NAME') are defined.\")\n",
    "    except Exception as e_viz:\n",
    "        print(f\"An error occurred during visualization of predictions: {e_viz}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nSkipping visualization of predictions due to unmet conditions or being disabled.\")\n",
    "\n",
    "print(\"\\nüèÅ Script execution completed! üèÅ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "70795b3b-fd0c-4ccf-9f02-4a4ef2eef810",
    "_uuid": "7a97a053-0e36-4e20-b4a2-bfdd7001c5d6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T15:08:51.232801Z",
     "iopub.status.busy": "2025-05-25T15:08:51.23252Z",
     "iopub.status.idle": "2025-05-25T15:08:51.59335Z",
     "shell.execute_reply": "2025-05-25T15:08:51.592402Z",
     "shell.execute_reply.started": "2025-05-25T15:08:51.232782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualization_model.predict('/kaggle/input/bank-checks-signatures-segmentation-dataset/TestSet/X/X_041.jpeg')[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61818eda-aaa0-474b-9af6-827b041ce803",
    "_uuid": "364ccb3b-555e-430d-a1fd-f2634f86bf79",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-25T15:43:08.018324Z",
     "iopub.status.busy": "2025-05-25T15:43:08.018129Z",
     "iopub.status.idle": "2025-05-25T15:43:12.52183Z",
     "shell.execute_reply": "2025-05-25T15:43:12.521084Z",
     "shell.execute_reply.started": "2025-05-25T15:43:08.01831Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "kagglehub.login()\n",
    "\n",
    "# Replace with path to directory containing model files.\n",
    "LOCAL_MODEL_DIR = '/kaggle/working/Signature_YOLOv10_Detection_Kaggle/yolov10s_epochs100_batch16/weights/best.pt'\n",
    "\n",
    "MODEL_SLUG = 'Yolo-with-signatures' # Replace with model slug.\n",
    "\n",
    "# Learn more about naming model variations at\n",
    "# https://www.kaggle.com/docs/models#name-model.\n",
    "VARIATION_SLUG = 'default' # Replace with variation slug.\n",
    "\n",
    "kagglehub.model_upload(\n",
    "  handle = f\"nguyenthien3001/{MODEL_SLUG}/pyTorch/{VARIATION_SLUG}\",\n",
    "  local_model_dir = LOCAL_MODEL_DIR,\n",
    "  version_notes = 'Update 2025-05-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T15:59:29.202144Z",
     "iopub.status.busy": "2025-05-25T15:59:29.201699Z",
     "iopub.status.idle": "2025-05-25T15:59:29.491461Z",
     "shell.execute_reply": "2025-05-25T15:59:29.490785Z",
     "shell.execute_reply.started": "2025-05-25T15:59:29.202122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch \n",
    "\n",
    "def crop_highest_confidence_signature(image_path, model_path, output_crop_path, signature_class_id=0):\n",
    "    \"\"\"\n",
    "    Detect signatures in an image, select the signature with the highest confidence,\n",
    "    crop it, and save it.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        model_path (str): Path to the trained YOLO model file (e.g., 'best.pt').\n",
    "        output_crop_path (str): Path to save the cropped signature image.\n",
    "        signature_class_id (int): ID of the 'signature' class (default is 0).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load the YOLO model\n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        model = YOLO(model_path)\n",
    "\n",
    "        # Check device and move model if necessary (e.g., if model was trained on GPU)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device)\n",
    "        print(f\"Model loaded on device: {device}\")\n",
    "\n",
    "        # 2. Perform prediction\n",
    "        print(f\"Performing prediction on image: {image_path}\")\n",
    "        # verbose=False to reduce log output, device=device to ensure running on the selected device\n",
    "        results = model.predict(source=image_path, verbose=False, device=device)\n",
    "\n",
    "        if not results or not results[0].boxes:\n",
    "            print(\"No objects detected in the image.\")\n",
    "            return\n",
    "\n",
    "        highest_confidence = 0.0\n",
    "        best_box_coords = None\n",
    "        found_signature = False\n",
    "\n",
    "        # results[0].boxes contains information about bounding boxes for the first (and only) image\n",
    "        # Important attributes: .xyxy (coordinates), .conf (confidence), .cls (class ID)\n",
    "        \n",
    "        # Move boxes to CPU for processing if on GPU\n",
    "        boxes_data = results[0].boxes.data.cpu().numpy()  # Convert tensor to numpy array on CPU\n",
    "\n",
    "        print(f\"Total number of detected objects: {len(boxes_data)}\")\n",
    "\n",
    "        for box_data in boxes_data:\n",
    "            # box_data is a row, e.g., [x1, y1, x2, y2, confidence, class_id]\n",
    "            x1, y1, x2, y2, conf, cls_id = box_data\n",
    "            \n",
    "            # 3. Filter by class_id and find the highest confidence\n",
    "            if int(cls_id) == signature_class_id:\n",
    "                found_signature = True\n",
    "                print(f\"  Detected signature with confidence: {conf:.4f}, coordinates: [{int(x1)}, {int(y1)}, {int(x2)}, {int(y2)}]\")\n",
    "                if conf > highest_confidence:\n",
    "                    highest_confidence = conf\n",
    "                    best_box_coords = (int(x1), int(y1), int(x2), int(y2))  # Store as tuple (int)\n",
    "\n",
    "        if not found_signature:\n",
    "            print(f\"No objects found belonging to the signature class (ID: {signature_class_id}).\")\n",
    "            return\n",
    "            \n",
    "        if best_box_coords:\n",
    "            print(f\"\\nSignature with highest confidence: {highest_confidence:.4f}\")\n",
    "            print(f\"Coordinates (x1, y1, x2, y2): {best_box_coords}\")\n",
    "\n",
    "            # 4. Crop the signature from the original image\n",
    "            original_image = Image.open(image_path)\n",
    "            cropped_signature = original_image.crop(best_box_coords)\n",
    "\n",
    "            # 5. Save the cropped signature image\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(output_crop_path), exist_ok=True)\n",
    "            cropped_signature.save(output_crop_path)\n",
    "            print(f\"Saved highest-confidence signature at: {output_crop_path}\")\n",
    "            \n",
    "            # Display the cropped image (optional, if running in an environment that supports display)\n",
    "            try:\n",
    "                from IPython.display import display\n",
    "                print(\"Cropped signature image:\")\n",
    "                display(cropped_signature)\n",
    "            except ImportError:\n",
    "                pass  # Skip if IPython.display is not available\n",
    "\n",
    "        else:\n",
    "            # This case should not occur if found_signature is True, but included for safety\n",
    "            print(\"No signatures found (after confidence check).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "if __name__ == '__main__':\n",
    "    # --- UPDATE THESE PATHS AS NEEDED ---\n",
    "    \n",
    "    # Assume the 'best.pt' model is in the results directory from the most recent training\n",
    "    # You need to provide the correct path based on PROJECT_SAVE_NAME and RUN_SAVE_NAME from the training script\n",
    "    # Example:\n",
    "    # PROJECT_SAVE_NAME_FROM_TRAIN = 'Signature_YOLOv10_Detection_Kaggle'\n",
    "    # RUN_SAVE_NAME_FROM_TRAIN = 'yolov10s_epochs100_batch16'  # Specific run name\n",
    "    # trained_model_path = f'/kaggle/working/{PROJECT_SAVE_NAME_FROM_TRAIN}/{RUN_SAVE_NAME_FROM_TRAIN}/weights/best.pt'\n",
    "    \n",
    "    # Or a fixed path if known\n",
    "    trained_model_path = '/kaggle/working/Signature_YOLOv10_Detection_Kaggle/yolov10s_epochs100_batch16/weights/best.pt'  # ‚ö†Ô∏è UPDATE THIS PATH\n",
    "\n",
    "    # Path to the image you want to process\n",
    "    # Example using an image from TestSet (if TestSet was merged into train, these are \"seen\" images)\n",
    "    # Or a completely new image\n",
    "    input_image_file = '/kaggle/input/bank-checks-signatures-segmentation-dataset/TestSet/X/X_018.jpeg'  # ‚ö†Ô∏è UPDATE THIS PATH\n",
    "\n",
    "    # Path to save the cropped signature\n",
    "    output_directory_for_crops = '/kaggle/working/cropped_signatures/'\n",
    "    # Create output filename based on input filename\n",
    "    base_img_name = os.path.splitext(os.path.basename(input_image_file))[0]\n",
    "    cropped_signature_file = os.path.join(output_directory_for_crops, f\"{base_img_name}_signature_cropped.png\")\n",
    "\n",
    "    # Check if model and image exist\n",
    "    if not os.path.exists(trained_model_path):\n",
    "        print(f\"ERROR: Model file not found at: {trained_model_path}\")\n",
    "        print(\"Please check the `trained_model_path`.\")\n",
    "    elif not os.path.exists(input_image_file):\n",
    "        print(f\"ERROR: Input image file not found at: {input_image_file}\")\n",
    "        print(\"Please check the `input_image_file`.\")\n",
    "    else:\n",
    "        # Call the function to process\n",
    "        crop_highest_confidence_signature(\n",
    "            image_path=input_image_file,\n",
    "            model_path=trained_model_path,\n",
    "            output_crop_path=cropped_signature_file,\n",
    "            signature_class_id=0  # Assume 'signature' class ID is 0\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1041694,
     "sourceId": 1753306,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
